{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e1e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vansh/whisper-stream/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import dualcodec\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import prepare_data\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59daa7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 58603.05it/s]\n",
      "/home/vansh/whisper-stream/.venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/vansh/.cache/huggingface/hub/models--amphion--dualcodec/snapshots/b5d3158cbd1007441794398435438228f1e80c28/dualcodec_12hz_16384_4096.safetensors\n",
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 69739.02it/s]\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 56552.41it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"12hz_v1\"\n",
    "dualcodec_model = dualcodec.get_model(model_id)\n",
    "dualcodec_inference = dualcodec.Inference(dualcodec_model=dualcodec_model, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5419f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac_model = dualcodec_inference.model.dac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(dualcodec_inference.decode))\n",
    "print(inspect.getsource(dualcodec_inference.model.decode_from_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = prepare_data(max_shards=1)\n",
    "audios = []\n",
    "sample_rates = []\n",
    "for i in range(10):                                          \n",
    "    audio = torch.from_numpy(ds[i][\"mp3\"][\"array\"]).float()  # type: ignore[attr-defined] \n",
    "    sample_rates.append(ds[i][\"mp3\"][\"sampling_rate\"])       # type: ignore[attr-defined]\n",
    "    audios.append(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, sr = torchaudio.load(\"tara.wav\")\n",
    "sample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=24000)(sample)\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_code, acoustic_code = dualcodec_inference.encode(sample.reshape(1,1,-1), n_quantizers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88889721",
   "metadata": {},
   "outputs": [],
   "source": [
    "dualcodec_inference.model.convnext_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c898cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(semantic_code[0][0]))\n",
    "print(max(acoustic_code[0][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(semantic_code.shape)\n",
    "print(acoustic_code.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eff0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the decoder codes 1-by-1 instead and collect the output samples to see if the decoder works\n",
    "def calculate_audio_with_receptive(semantic_code, acoustic_code, look_ahead, look_back):\n",
    "    my_audio = np.array([])\n",
    "    num_codes = len(semantic_code[0][0])\n",
    "    assert num_codes == len(acoustic_code[0][0])\n",
    "\n",
    "    for i in range(0, num_codes):\n",
    "        l = max(i - look_back, 0)\n",
    "        r = min(i + look_ahead, num_codes)\n",
    "\n",
    "        # print(f\"looking back {l} tokens, and looking ahead {r} tokens\")\n",
    "\n",
    "        sm = semantic_code[:, :, l:r]\n",
    "        ac = acoustic_code[:, :, l:r]\n",
    "\n",
    "        # print(f\"num codes given: {r-l}\")\n",
    "        # print(f\"num samples generated: {out_audio.shape[0]}\")\n",
    "\n",
    "        out_audio = dualcodec_inference.decode(sm, ac)\n",
    "        out_audio = out_audio.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "        space = l * 1920\n",
    "\n",
    "        # print(l, r, len(out_audio), i * 1920 - space)\n",
    "\n",
    "        my_audio = np.concatenate([my_audio, out_audio[1920 * i - space : 1920 * (i+1) - space]])\n",
    "        \n",
    "    return my_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original audio\n",
    "Audio(sample.squeeze(0).squeeze(0).cpu().numpy(), rate=24000)\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_streamed_audio = dualcodec_inference.decode(semantic_code, acoustic_code).squeeze(0).cpu().numpy()\n",
    "print(non_streamed_audio.shape)\n",
    "Audio(non_streamed_audio, rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_1 = calculate_audio_with_receptive(semantic_code, acoustic_code, 10, 10)\n",
    "# stream_2 = calculate_audio_with_receptive(semantic_code, acoustic_code, 20, 20)\n",
    "# stream_3 = calculate_audio_with_receptive(semantic_code, acoustic_code, 30, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23981240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_4 = calculate_audio_with_receptive(semantic_code, acoustic_code, 40, 40)\n",
    "# stream_5 = calculate_audio_with_receptive(semantic_code, acoustic_code, 50, 50)\n",
    "# stream_6 = calculate_audio_with_receptive(semantic_code, acoustic_code, 60, 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_7 = calculate_audio_with_receptive(semantic_code, acoustic_code, 70, 70)\n",
    "# stream_8 = calculate_audio_with_receptive(semantic_code, acoustic_code, 80, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_9 = calculate_audio_with_receptive(semantic_code, acoustic_code, 5, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_8 = calculate_audio_with_receptive(semantic_code, acoustic_code, 5, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stream_8, label=\"stream_8\")\n",
    "plt.plot(stream_9, label=\"stream_9\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2789ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(stream_9, rate=24000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(stream_8, rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6450426",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = [\n",
    "    stream_8,\n",
    "    stream_9,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, stream in enumerate(streams):\n",
    "    differences = stream - non_streamed_audio[0]\n",
    "    plt.plot(differences, label=f\"stream_{j+1}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f60a88",
   "metadata": {},
   "source": [
    "### DAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437cbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac_inputs = torch.randn(1, 1024, 252, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ae18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_stream_dac = dac_model.decoder(dac_inputs).squeeze(0).squeeze(0).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f10692",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac_model.decoder_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol(inputs, model_now, look_ahead, look_back, space_cons):\n",
    "    my_audio = np.array([])\n",
    "    num_codes = len(inputs[0][0])\n",
    "    \n",
    "\n",
    "    for i in range(0, num_codes):\n",
    "        l = max(i - look_back, 0)\n",
    "        r = min(i + look_ahead, num_codes)\n",
    "        \n",
    "        dac_inputs = inputs[:, :, l:r]\n",
    "\n",
    "        out_audio = model_now(dac_inputs)\n",
    "        out_audio = out_audio.squeeze(0).squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "        space = l * space_cons\n",
    "        \n",
    "        # print(f\"num samples generated: {out_audio.shape}\")\n",
    "        # print(f\"my audio shape: {my_audio.shape}\")\n",
    "        \n",
    "        my_audio = np.concatenate([\n",
    "            my_audio, \n",
    "            out_audio[space_cons * i - space : space_cons * (i+1) - space]\n",
    "            ])\n",
    "        \n",
    "        # print(f\"num samples added so far: {my_audio.shape[0]} at loop {i}\")\n",
    "        \n",
    "    return my_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ff335",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamed_dac_audio = lol(dac_inputs, dac_model.decoder, 65, 55, 1920);\n",
    "streamed_dac_audio_2 = lol(dac_inputs, dac_model.decoder, 70, 55, 1920);\n",
    "streamed_dac_audio_3 = lol(dac_inputs, dac_model.decoder, 75, 55, 1920);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73699fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamed_dac_audio_4 = lol(dac_inputs, dac_model.decoder, 55, 75, 1920);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_stream_dac.shape)\n",
    "print(streamed_dac_audio.shape)\n",
    "print(streamed_dac_audio_2.shape)\n",
    "print(streamed_dac_audio_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(streamed_dac_audio_2, label=\"streamed\")\n",
    "plt.show()\n",
    "plt.plot(non_stream_dac, label=\"non-streamed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(streamed_dac_audio_4 - non_stream_dac, label=\"diff\")\n",
    "plt.vlines(x=1920 * 75 - 4, ymin=-0.001, ymax=0.001, color=\"red\")\n",
    "plt.vlines(x=1920 * (252 - 55) - 4, ymin=-0.001, ymax=0.001, color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c52bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = dac_model.decoder.model\n",
    "print(layers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol(inputs, model_now, look_ahead, look_back, space_cons, num_frames):\n",
    "    my_audio = np.zeros((inputs.shape[0], inputs.shape[1], num_frames))\n",
    "    \n",
    "    for i in range(0, num_frames):\n",
    "        l = max(i - look_back, 0)\n",
    "        r = min(i + look_ahead, num_frames)\n",
    "        \n",
    "        dac_inputs = inputs[:, :, l:r]\n",
    "\n",
    "        out_audio = model_now(dac_inputs)\n",
    "        out_audio = out_audio.cpu().detach().numpy()\n",
    "        \n",
    "        print(out_audio.shape)\n",
    "\n",
    "        space = l * space_cons\n",
    "        \n",
    "        my_audio[:, :, i] = out_audio[:, :, space_cons * i - space : space_cons * (i+1) - space]\n",
    "        \n",
    "    return my_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac_outputs_non_stream_1 = layers[0](dac_inputs)\n",
    "dac_outputs_stream_1 = lol(dac_inputs, layers[0], 1, 1, 1920, dac_inputs.shape[2])\n",
    "\n",
    "\n",
    "\n",
    "print(dac_outputs_non_stream_1.shape)\n",
    "print(dac_outputs_stream_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a64e9d",
   "metadata": {},
   "source": [
    "### Wav2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19bf61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = dualcodec_inference.semantic_cfg.feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fef8a12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeamlessM4TFeatureExtractor {\n",
      "  \"feature_extractor_type\": \"SeamlessM4TFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 1,\n",
      "  \"processor_class\": \"Wav2Vec2BertProcessor\",\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000,\n",
      "  \"stride\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc3a38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000,)\n",
      "(160000,)\n"
     ]
    }
   ],
   "source": [
    "audio_5s = torch.randn(16000 * 5).numpy()\n",
    "audio_7s = np.concatenate((audio_5s, torch.randn(16000 * 5).numpy()))\n",
    "\n",
    "print(audio_5s.shape)\n",
    "print(audio_7s.shape)\n",
    "\n",
    "o1 = new_model(audio_5s, sampling_rate=16000)[\"input_features\"][0]\n",
    "o2 = new_model(audio_7s, sampling_rate=16000)[\"input_features\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c59fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 160)\n",
      "(499, 160)\n"
     ]
    }
   ],
   "source": [
    "print(o1.shape)\n",
    "print(o2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5afd88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5893896  -0.66483647 -0.4504154  -1.3175714  -0.68299586 -0.02689089\n",
      "  0.27578333  0.03610433 -0.01030014 -0.20770043]\n",
      "--------------------------------\n",
      "[-1.4899344  -0.6980322  -0.4099009  -1.320732   -0.7294301  -0.05279103\n",
      "  0.2647866  -0.01022197 -0.00367124 -0.17570677]\n"
     ]
    }
   ],
   "source": [
    "print(o1[0][:10])\n",
    "print(\"--------------------------------\")\n",
    "print(o2[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d9919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
