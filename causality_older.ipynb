{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f74aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plotter   # type: ignore[attr-defined]\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-base\"    \n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "processor = WhisperProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(model.model.forward))\n",
    "# print(inspect.getsource(model.model.forward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60074321",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.model.encoder.layers:        \n",
    "    layer.self_attn.is_causal = True            # type: ignore[attr-defined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_5s = torch.randn(16000 * 10).numpy()\n",
    "audio_7s = np.concatenate([audio_5s, torch.randn(16000 * 6).numpy()])\n",
    "\n",
    "inputs_5s = processor(audio_5s, sampling_rate=16000, return_tensors=\"pt\") # type: ignore[attr-defined]\n",
    "inputs_7s = processor(audio_7s, sampling_rate=16000, return_tensors=\"pt\") # type: ignore[attr-defined]\n",
    "\n",
    "print(f\"Mel spectrogram 5s: {inputs_5s.input_features.shape}\")\n",
    "print(f\"Mel spectrogram 7s: {inputs_7s.input_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a873c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, axes = plotter.subplots(5, 1, figsize=(15, 12))\n",
    "\n",
    "for i in range(5):\n",
    "    feature_5s = inputs_5s.input_features[0, i, :].numpy()\n",
    "    feature_7s = inputs_7s.input_features[0, i, :].numpy()\n",
    "    \n",
    "    axes[i].plot(feature_5s, label='5s audio', alpha=0.7, linewidth=1)\n",
    "    axes[i].plot(feature_7s, label='7s audio', alpha=0.7, linewidth=1)\n",
    "    axes[i].set_title(f'Mel Frequency Bin {i}')\n",
    "    axes[i].set_xlabel('Time Frame')\n",
    "    axes[i].set_ylabel('Mel Value')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plotter.tight_layout()\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba818e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5s = inputs_5s.input_features\n",
    "feature_7s = inputs_7s.input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels_before_attention   = feature_5s.shape[-1] // 2    # 3000 â†’ 1500 after conv\n",
    "\n",
    "# causal    = torch.tril(torch.ones(channels_before_attention, channels_before_attention))\n",
    "\n",
    "# attn_mask = torch.where(causal == 1,\n",
    "#                         torch.tensor(0.0),\n",
    "#                         torch.tensor(float(\"-inf\"))\n",
    "#                        )[None, None, :, :]     \n",
    "\n",
    "# attn_mask = attn_mask.to(dtype=model.model.encoder.embed_positions.weight.dtype)\n",
    "\n",
    "# model.eval()\n",
    "# # Generate transcription\n",
    "# with torch.no_grad():\n",
    "#     latents_5s = model.model.encoder(feature_5s, attention_mask=attn_mask)\n",
    "#     latents_7s = model.model.encoder(feature_7s, attention_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21662006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process both through the encoder\n",
    "with torch.no_grad():\n",
    "    latents_5s = model.model.encoder(feature_5s)\n",
    "    latents_7s = model.model.encoder(feature_7s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_5s_tensor = latents_5s.last_hidden_state\n",
    "latents_7s_tensor = latents_7s.last_hidden_state\n",
    "\n",
    "print(f\"5s latents shape: {latents_5s_tensor.shape}\")\n",
    "print(f\"7s latents shape: {latents_7s_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot(latents_5s_tensor[0, 1099, :])\n",
    "plotter.plot(latents_7s_tensor[0, 1099, :])\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45479d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1500):\n",
    "    l1 = latents_5s_tensor[0][i]\n",
    "    l2 = latents_7s_tensor[0][i]\n",
    "    if not np.allclose(l1, l2):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf68c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latents_5s_tensor[0][150])\n",
    "print(\"hello\")\n",
    "print(latents_7s_tensor[0][450])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a530f",
   "metadata": {},
   "source": [
    "### Wav2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f86924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(model.model.encoder.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c851f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"facebook/w2v-bert-2.0\"\n",
    "model = Wav2Vec2BertModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of encoder layers: {len(model.encoder.layers)}\")\n",
    "\n",
    "lennn = 500\n",
    "\n",
    "audio_5s = torch.randn((1, 4, 160))\n",
    "audio_7s = np.concatenate((audio_5s.numpy(), torch.randn((1, 4, 160)).numpy()), axis=1)\n",
    "audio_7s = torch.from_numpy(audio_7s)\n",
    "\n",
    "print(f\"A: {audio_5s.shape}\")\n",
    "print(f\"Long tensor shape: {audio_7s.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2928945",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    o1 = model(audio_5s)\n",
    "    o2 = model(audio_7s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o1.last_hidden_state.shape)\n",
    "print(o2.last_hidden_state.shape)\n",
    "\n",
    "o1h = o1.last_hidden_state\n",
    "o2h = o2.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(o1h[0][i])\n",
    "    print(o2h[0][i])\n",
    "    print(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
