{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f74aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plotter   # type: ignore[attr-defined]\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2BertModel, AutoProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoFeatureExtractor, Wav2Vec2BertModel\n",
    "# import torch\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "# dataset = dataset.sort(\"id\")\n",
    "# sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "# model = Wav2Vec2BertModel.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "\n",
    "# # audio file is decoded on the fly\n",
    "# inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-base\"    \n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "processor = WhisperProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(model.model.encoder.layers[0].self_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60074321",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.model.encoder.layers:        \n",
    "    layer.self_attn.is_causal = True            # type: ignore[attr-defined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_5s = torch.randn(16000 * 10).numpy()\n",
    "audio_7s = np.concatenate([audio_5s, torch.randn(16000 * 6).numpy()])\n",
    "\n",
    "inputs_5s = processor(audio_5s, sampling_rate=16000, return_tensors=\"pt\") # type: ignore[attr-defined]\n",
    "inputs_7s = processor(audio_7s, sampling_rate=16000, return_tensors=\"pt\") # type: ignore[attr-defined]\n",
    "\n",
    "print(f\"Mel spectrogram 5s: {inputs_5s.input_features.shape}\")\n",
    "print(f\"Mel spectrogram 7s: {inputs_7s.input_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a873c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, axes = plotter.subplots(5, 1, figsize=(15, 12))\n",
    "\n",
    "for i in range(5):\n",
    "    feature_5s = inputs_5s.input_features[0, i, :].numpy()\n",
    "    feature_7s = inputs_7s.input_features[0, i, :].numpy()\n",
    "    \n",
    "    axes[i].plot(feature_5s, label='5s audio', alpha=0.7, linewidth=1)\n",
    "    axes[i].plot(feature_7s, label='7s audio', alpha=0.7, linewidth=1)\n",
    "    axes[i].set_title(f'Mel Frequency Bin {i}')\n",
    "    axes[i].set_xlabel('Time Frame')\n",
    "    axes[i].set_ylabel('Mel Value')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plotter.tight_layout()\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba818e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5s = inputs_5s.input_features\n",
    "feature_7s = inputs_7s.input_features\n",
    "\n",
    "tgt_len   = feature_5s.shape[-1] // 2                 # 3000 â†’ 1500 after conv\n",
    "\n",
    "# start with lower-triangular 0/-inf\n",
    "causal    = torch.tril(torch.ones(tgt_len, tgt_len))\n",
    "\n",
    "attn_mask = torch.where(causal == 1,\n",
    "                        torch.tensor(0.0),\n",
    "                        torch.tensor(float(\"-inf\"))\n",
    "                       )[None, None, :, :]          # (1,1,tgt,src) additive mask\n",
    "\n",
    "attn_mask = attn_mask.to(dtype=model.model.encoder.embed_positions.weight.dtype)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "# Generate transcription\n",
    "with torch.no_grad():\n",
    "    latents_5s = model.model.encoder(feature_5s, attention_mask=attn_mask)\n",
    "    latents_7s = model.model.encoder(feature_7s, attention_mask=attn_mask)\n",
    "    \n",
    "\n",
    "# # Process both through the encoder\n",
    "# with torch.no_grad():\n",
    "#     latents_5s = model.model.encoder(feature_5s)\n",
    "#     latents_7s = model.model.encoder(feature_7s)\n",
    "    \n",
    "\n",
    "latents_5s_tensor = latents_5s.last_hidden_state\n",
    "latents_7s_tensor = latents_7s.last_hidden_state\n",
    "\n",
    "print(f\"5s latents shape: {latents_5s_tensor.shape}\")\n",
    "print(f\"7s latents shape: {latents_7s_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45479d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1500):\n",
    "    l1 = latents_5s_tensor[0][i]\n",
    "    l2 = latents_7s_tensor[0][i]\n",
    "    if not np.allclose(l1, l2):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf68c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latents_5s_tensor[0][150])\n",
    "print(\"hello\")\n",
    "print(latents_7s_tensor[0][450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f86924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(model.model.encoder.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b96e3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c851f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_id = \"facebook/w2v-bert-2.0\"\n",
    "model = Wav2Vec2BertModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d30427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Wav2Vec2BertModel.forward of Wav2Vec2BertModel(\n",
      "  (feature_projection): Wav2Vec2BertFeatureProjection(\n",
      "    (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "    (projection): Linear(in_features=160, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): Wav2Vec2BertEncoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Wav2Vec2BertEncoderLayer(\n",
      "        (ffn1_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn1): Wav2Vec2BertFeedForward(\n",
      "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (intermediate_act_fn): SiLU()\n",
      "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (self_attn): Wav2Vec2BertSelfAttention(\n",
      "          (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (distance_embedding): Embedding(73, 64)\n",
      "        )\n",
      "        (conv_module): Wav2Vec2BertConvolutionModule(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (glu): GLU(dim=1)\n",
      "          (depthwise_conv): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), groups=1024, bias=False)\n",
      "          (depthwise_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): SiLU()\n",
      "          (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ffn2_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn2): Wav2Vec2BertFeedForward(\n",
      "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (intermediate_act_fn): SiLU()\n",
      "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ba5329d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encoder layers: 24\n",
      "A: torch.Size([1, 4, 160])\n",
      "Long tensor shape: torch.Size([1, 8, 160])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of encoder layers: {len(model.encoder.layers)}\")\n",
    "\n",
    "lennn = 500\n",
    "\n",
    "audio_5s = torch.randn((1, 4, 160))\n",
    "audio_7s = np.concatenate((audio_5s.numpy(), torch.randn((1, 4, 160)).numpy()), axis=1)\n",
    "audio_7s = torch.from_numpy(audio_7s)\n",
    "\n",
    "print(f\"A: {audio_5s.shape}\")\n",
    "print(f\"Long tensor shape: {audio_7s.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2928945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    o1 = model(audio_5s)\n",
    "    o2 = model(audio_7s)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db68e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1024])\n",
      "torch.Size([1, 8, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(o1.last_hidden_state.shape)\n",
    "print(o2.last_hidden_state.shape)\n",
    "\n",
    "o1h = o1.last_hidden_state\n",
    "o2h = o2.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c99f6a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0662,  0.0496, -0.0166,  ...,  0.0097,  0.0438,  0.0263])\n",
      "tensor([ 0.0570,  0.0727,  0.0057,  ...,  0.0039,  0.0041, -0.0060])\n",
      "--------------------------------\n",
      "tensor([ 0.0227, -0.0179,  0.0021,  ..., -0.0771,  0.0905,  0.1242])\n",
      "tensor([-0.0341, -0.0060,  0.0105,  ..., -0.0727,  0.0736,  0.1535])\n",
      "--------------------------------\n",
      "tensor([ 0.0565, -0.0304, -0.0194,  ..., -0.0580,  0.0580,  0.0551])\n",
      "tensor([ 0.0845, -0.0100, -0.0025,  ..., -0.0313,  0.0206,  0.0607])\n",
      "--------------------------------\n",
      "tensor([ 0.0040,  0.0202, -0.0066,  ...,  0.0107, -0.0203,  0.0550])\n",
      "tensor([ 0.0687, -0.0048,  0.0013,  ..., -0.0200,  0.0064,  0.0125])\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(o1h[0][i])\n",
    "    print(o2h[0][i])\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceddf05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
